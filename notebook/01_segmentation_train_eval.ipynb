{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01_segmentation_train_eval.ipynb\n",
    "\n",
    "This file performs the model training process of our Pix2Pix-based lung segmentation system.\n",
    "Additionally, it performs model evaluation based on a seperately defined test dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Define directories, import required libraries and setup sesion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Change working directory to the root folder\n",
    "import os, sys\n",
    "if os.path.split(os.getcwd())[-1] != 'SmartDetect_segmentation':\n",
    "    %cd ..\n",
    "    sys.path.append(\"src\")\n",
    "    \n",
    "    if os.path.split(os.getcwd())[-1] != 'SmartDetect_segmentation':\n",
    "        raise UserError(\"Something went wrong in the directory reassignment!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Perform required imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from preprocessing import preprocessing\n",
    "from dataset import generate_dataset\n",
    "from model import define_discriminator, define_generator, define_gan\n",
    "from training import *\n",
    "from evaluation import *\n",
    "from util.general import *\n",
    "from util.inspection import *\n",
    "from util.tf_session import *\n",
    "\n",
    "# Setup GPU tensorflow session\n",
    "n_gpus = setup_tf_gpu_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define some basic directories to use in the rest of this program\n",
    "dataDir = os.path.join(\"data\", \"preprocessed\")\n",
    "modelDir = \"model\"\n",
    "notebookDir = \"notebook\"\n",
    "logDir = \"logs\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Perform data preprocessing\n",
    "Please note that since this part may take quite a long time, it is skipped on default if the software detects preprocessed data already."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing the data. \n",
    "# To rerun the preprocessing, change 'rerun' to True\n",
    "\n",
    "preprocessing(rerun=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3: Dataset generation\n",
    "The data is split into a training and test set per default.\n",
    "Validation is performed with the training set based on a later split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = generate_dataset(dataDir, split_dataset=True, train_or_test='train')\n",
    "dataset_test = generate_dataset(dataDir, split_dataset=True, train_or_test='test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Visual data inspection\n",
    "Here, we will also briefly inspect the data we'll be training the model with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, image_shape = inspect_dataset(dataset_train, 'train')\n",
    "_, _ = inspect_dataset(dataset_test, 'test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4: Model definition\n",
    "Here, we will define the GAN model we'll be using for the segmentation purposes.\n",
    "It is derived from the Pix2Pix model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_shape = (image_shape[0], image_shape[1], 1)\n",
    "\n",
    "g_model = define_generator(image_shape)\n",
    "d_model = define_discriminator(image_shape)\n",
    "gan_model = define_gan(g_model, d_model, image_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 5: Monitoring\n",
    "The training process is monitored via TensorBoard.\n",
    "The results will be displayed here by default. Note, however, that we may also monitor the process manually or after training time by opening tensorboard via the terminal as such:\n",
    "\n",
    "`tensorboard --logdir \"logs\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext tensorboard\n",
    "# %tensorboard --logdir {logDir}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 6: Training\n",
    "Here, the actual training process is performed. \n",
    "We may pass some hyperparameters in the 'train' function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "_, last_save = train(d_model, g_model, gan_model, dataset_train) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 7: Evaluation\n",
    "Here, we perform the evaluation of our model, based on the previously defined test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a trained model\n",
    "model = load_model(last_save)\n",
    "\n",
    "# Evaluate it on the test set\n",
    "print(\"\\n\\n===== EVALUATION METRICS =====\")\n",
    "evaluate_model(dataset_test, model)\n",
    "\n",
    "# Or... put dataset through a model and get predicted images\n",
    "print(\"\\n\\n===== DATASET PREDICTION =====\")\n",
    "predicted_images = predict_dataset(dataset_test, model)\n",
    "print(f\"{np.shape(predicted_images)[0]} images predicted!\")\n",
    "\n",
    "# Or... put a single image through the model and plot the prediction\n",
    "print(\"\\n\\n====== IMAGE PREDICTION ======\")\n",
    "for i in range(np.shape(dataset_test)[1] // 5):  # 1/5 of the test set displayed\n",
    "    print(f\"\\n--- SUBJECT {i:3d} ---\")\n",
    "    predict_and_plot_image(dataset_test[0][i], model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
